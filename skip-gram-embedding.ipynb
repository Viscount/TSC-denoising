{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21573874, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "\n",
    "seasons = pd.read_csv(\"./data/bangumi.csv\", delimiter=\",\", encoding=\"utf-8\")\n",
    "episodes = pd.read_csv(\"./data/episode.csv\", delimiter=\",\", encoding=\"utf-8\")\n",
    "danmaku_complete = pd.read_csv(\"./data/danmaku_complete.csv\", delimiter=\"\\t\", encoding=\"utf-8\", quoting=csv.QUOTE_NONE, low_memory=False)\n",
    "danmaku_complete = danmaku_complete.fillna(-1)\n",
    "danmaku_complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as segtool\n",
    "import re\n",
    "\n",
    "ACCEPTABLE_TYPE = {'n', 't', 's', 'f', 'v', 'a', 'b', 'z', 'e', 'y', 'o'}\n",
    "REPLACE_DICT = {\n",
    "    \"233+\": \"233\",\n",
    "    \"666+\": \"666\"\n",
    "}\n",
    "\n",
    "def check_type(word_type):\n",
    "    if word_type[0] in ACCEPTABLE_TYPE:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_replace(word):\n",
    "    for item in REPLACE_DICT.keys():\n",
    "        pattern = re.compile(item)\n",
    "        if re.match(pattern, word) is not None:\n",
    "            new_word = REPLACE_DICT[item]\n",
    "            return new_word\n",
    "    return word\n",
    "\n",
    "def word_segment(content):\n",
    "    words = []\n",
    "    results = segtool.cut(content)\n",
    "    for result in results:\n",
    "        result.word = check_replace(result.word)\n",
    "        if check_type(result.flag):\n",
    "            words.append(result.word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "\n",
    "class DmDataset(data.Dataset):\n",
    "    def __init__(self, dm_samples, context_size, min_count, neg_sampling_num):\n",
    "        self.neg_sampling_num = neg_sampling_num\n",
    "        print('building vocabulary...')\n",
    "        aggregate_sample = []\n",
    "        for sample in dm_samples:\n",
    "            aggregate_sample.extend(sample)\n",
    "        counter = {'UNK': 0}\n",
    "        counter.update(collections.Counter(aggregate_sample).most_common())\n",
    "        rare_words = set()\n",
    "        for word in counter:\n",
    "            if word != 'UNK' and counter[word] <= min_count:\n",
    "                rare_words.add(word)\n",
    "        for word in rare_words:\n",
    "            counter['UNK'] += counter[word]\n",
    "            counter.pop(word)\n",
    "        print('%d words founded in vocabulary' % len(counter))\n",
    "        \n",
    "        self.vocab_counter = counter\n",
    "        self.word_to_ix = dict()\n",
    "        for word in counter:\n",
    "            self.word_to_ix[word] = len(self.word_to_ix)\n",
    "            \n",
    "        counts = [self.vocab_counter[key] for key in self.vocab_counter]\n",
    "        frequency = np.array(counts)/sum(counts)\n",
    "        self.subsampling_P = dict()\n",
    "        for idx, x in enumerate(frequency):\n",
    "            y = (math.sqrt(x/0.001)+1)*0.001/x\n",
    "            self.subsampling_P[idx] = y\n",
    "            \n",
    "        pow_frequency = np.array(counts)**0.75\n",
    "        power = sum(pow_frequency)\n",
    "        self.neg_sampling_ratio = pow_frequency/ power\n",
    "        \n",
    "        print('building samples...')\n",
    "        self.samples = []\n",
    "        span = context_size*2+1\n",
    "        for sample in dm_samples:\n",
    "            # skip heading and tailing words\n",
    "            # subsampling\n",
    "            sample_ = []\n",
    "            for word in sample:\n",
    "                word_ix = self.word2ix(word)\n",
    "                if random.random() < self.subsampling_P[word_ix]:\n",
    "                    sample_.append(word_ix)\n",
    "            # generate word pair\n",
    "            start_index = 0\n",
    "            done = False\n",
    "            while start_index + span <= len(sample_):\n",
    "                buffer = sample_[start_index: start_index + span]\n",
    "                done = True\n",
    "                target_word = buffer[context_size]\n",
    "                for index in range(0, len(buffer)):\n",
    "                    if index != context_size:\n",
    "                        self.samples.append((target_word, buffer[index]))\n",
    "                start_index += 1\n",
    "            if not done:\n",
    "                buffer = sample_[:]\n",
    "                if len(buffer)>1:\n",
    "                    target_word = buffer[len(buffer)//2]\n",
    "                    for index in range(0, len(buffer)):\n",
    "                        if index != len(buffer)//2:\n",
    "                            self.samples.append((target_word, buffer[index]))\n",
    "    \n",
    "        print('%d samples constructed.' % len(self.samples))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def word2ix(self, word):\n",
    "        if word in self.word_to_ix:\n",
    "            return self.word_to_ix[word]\n",
    "        else:\n",
    "            return self.word_to_ix['UNK']\n",
    " \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        target = sample[0]\n",
    "        context = sample[1]\n",
    "        neg_samples = np.random.choice(len(self.vocab_counter), self.neg_sampling_num, \n",
    "                                       p=self.neg_sampling_ratio)\n",
    "        sample_dict = {\n",
    "            'pos_u': target,\n",
    "            'pos_v': context,\n",
    "            'neg_v': np.array(neg_samples)\n",
    "        }\n",
    "        return sample_dict\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    \n",
    "    def save_vocab(self, path):\n",
    "        vocab = []\n",
    "        for word in self.vocab_counter:\n",
    "            vocab.append({'idx': self.word_to_ix[word],\n",
    "                          'word': word,\n",
    "                          'count': self.vocab_counter[word]})\n",
    "        df = pd.DataFrame(vocab)\n",
    "        df.to_csv(path, index=False)\n",
    "        return\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab_counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocabulary...\n",
      "2877 words founded in vocabulary\n",
      "building samples...\n",
      "81203 samples constructed.\n"
     ]
    }
   ],
   "source": [
    "danmaku_selected = danmaku_complete[danmaku_complete['episode_id']==173248]\n",
    "\n",
    "context_size = 3\n",
    "min_count = 5\n",
    "neg_sampling_num = 10\n",
    "samples = []\n",
    "\n",
    "for index, row in danmaku_selected.iterrows():\n",
    "    content = row['content']\n",
    "    words = word_segment(content)\n",
    "    samples.append(words)\n",
    "\n",
    "dm_set = DmDataset(samples, context_size, min_count, neg_sampling_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as V\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramLanguageModeler, self).__init__()\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)   \n",
    "        self.v_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True) \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.init_emb()\n",
    "        \n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.embedding_dim\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, u_pos, v_pos, v_neg, batch_size):\n",
    "        embed_u = self.u_embeddings(u_pos)\n",
    "        embed_v = self.v_embeddings(v_pos)\n",
    "\n",
    "        score  = torch.mul(embed_u, embed_v)\n",
    "        score = torch.sum(score, dim=1)\n",
    "        log_target = F.logsigmoid(score).squeeze()\n",
    "    \n",
    "        neg_embed_v = self.v_embeddings(v_neg)\n",
    "    \n",
    "        neg_score = torch.bmm(neg_embed_v, embed_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.sum(neg_score, dim=1)\n",
    "        sum_log_sampled = F.logsigmoid(-1*neg_score).squeeze()\n",
    "\n",
    "        loss = log_target + sum_log_sampled\n",
    "\n",
    "        return -1*loss.sum()/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGramLanguageModeler (\n",
      "  (u_embeddings): Embedding(2877, 200, sparse=True)\n",
      "  (v_embeddings): Embedding(2877, 200, sparse=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "def plot_durations(y):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.plot(y)\n",
    "\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "torch.manual_seed(1) \n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "batch_size=128\n",
    "epoch_num = 10\n",
    "\n",
    "dm_dataloader = Data.DataLoader(\n",
    "    dataset=dm_set,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "model = SkipGramLanguageModeler(dm_set.vocab_size(), EMBEDDING_DIM)\n",
    "print(model)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.3)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for batch_idx, sample in enumerate(dm_dataloader):\n",
    "        pos_u = Variable(torch.LongTensor(sample['pos_u']))\n",
    "        pos_v = Variable(torch.LongTensor(sample['pos_v']))\n",
    "        neg_v = Variable(torch.LongTensor(sample['neg_v']))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pos_u = pos_u.cuda()\n",
    "            pos_v = pos_v.cuda()\n",
    "            neg_v = neg_v.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(pos_u, pos_v, neg_v, batch_size)\n",
    "        print('epoch: %d batch %d : loss: %4.4f' % (epoch, batch_idx, loss.data[0]))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
